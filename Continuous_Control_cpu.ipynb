{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "False\n",
      "1.10.0+cpu\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "#DEVICE = torch.device('cuda:0')\n",
    "ROLLOUT_LENGTH = 250 \n",
    "DISCOUNT = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "OPTIMIZATION_EPOCHS = 10\n",
    "MINI_BATCH_SIZE = 64\n",
    "PPO_RATIO_CLIP = 0.1\n",
    "GRADIENT_CLIP = 0.75\n",
    "HIDDEN_LAYERS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-party Code\n",
    "These are helper routines copied or adapted from other projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to Shangtong Zhang \n",
    "# https://github.com/ShangtongZhang/DeepRL\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "def to_np(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "def tensor(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = torch.tensor(x, device=DEVICE, dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]\n",
    "\n",
    "class MeanStdNormalizer():\n",
    "    def __init__(self):\n",
    "        self.rms = None\n",
    "        self.clip = 10.0\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.rms is None:\n",
    "            self.rms = RunningMeanStd(shape=(1, ) + x.shape[1:])\n",
    "        self.rms.update(x)\n",
    "        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n",
    "                       -self.clip, self.clip)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to OpenAI.\n",
    "\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/running_mean_std.py    \n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)    \n",
    "        \n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Agent\n",
    "\n",
    "The master agent implements the PPO algorithm and can use multiple sub-agents for the purpose of samlping trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class SubNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, output_size, seed):\n",
    "        super(SubNetwork, self).__init__()\n",
    "        dims = (input_size,) + hidden_units        \n",
    "        self.layers = nn.ModuleList([layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.feature_dim = dims[-1]\n",
    "        self.output_layer = layer_init(nn.Linear(self.feature_dim, output_size), 1e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.tanh(layer(x))\n",
    "        x = self.output_layer(x)    \n",
    "        return x    \n",
    "            \n",
    "class ActorAndCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        super(ActorAndCritic, self).__init__()\n",
    "        self.seed = random.seed(seed)\n",
    "        self.actor = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), action_size, seed)\n",
    "        self.critic = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), 1, seed)\n",
    "        self.std = nn.Parameter(torch.zeros(action_size))\n",
    "        #self.to(Config.DEVICE)\n",
    "        \n",
    "    def forward(self, obs, action=None):\n",
    "        obs = tensor(obs)\n",
    "        a = self.actor(obs)\n",
    "        v = self.critic(obs)\n",
    "        mean = F.tanh(a)\n",
    "        dist = torch.distributions.Normal(mean, F.softplus(self.std))\n",
    "        return (v, dist)\n",
    "        \n",
    "class Rollout():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stored values\n",
    "        self.actions = []\n",
    "        self.log_prob_actions = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.episode_not_dones = []\n",
    "        self.states = []\n",
    "        # Calculated values\n",
    "        self.returns = [0.0] * ROLLOUT_LENGTH\n",
    "        self.advantages = [0.0] * ROLLOUT_LENGTH\n",
    "        \n",
    "    def save_prediction(self, actions, log_prob_actions, values):\n",
    "        self.actions.append(actions)\n",
    "        self.log_prob_actions.append(log_prob_actions)\n",
    "        self.values.append(values)\n",
    "\n",
    "    def save_consequences(self, rewards, episode_not_dones, states):\n",
    "        self.rewards.append(rewards)\n",
    "        self.episode_not_dones.append(episode_not_dones)\n",
    "        self.states.append(states)\n",
    "        \n",
    "    def calculate_returns_and_advantages(self, final_reward):\n",
    "        self.rewards.append(None)\n",
    "        self.episode_not_dones.append(None)\n",
    "        self.calculate_future_returns(final_reward)\n",
    "        self.estimate_advantages()\n",
    "\n",
    "    def calculate_future_returns(self, returns):\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            returns = self.rewards[i] + DISCOUNT * self.episode_not_dones[i] * returns\n",
    "            self.returns[i] = returns.detach() \n",
    "\n",
    "    def estimate_advantages(self):\n",
    "        advantages = tensor(np.zeros((num_agents, 1)))\n",
    "        # Go backwards through rollout steps and calculate advantages for each state action pair\n",
    "        # Use GAE for PPO. (Schulman, Moritz, Levine et al. 2016)\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            td = self.rewards[i] + (DISCOUNT * self.episode_not_dones[i] * self.values[i + 1]) - self.values[i]\n",
    "            advantages = advantages * GAE_LAMBDA * DISCOUNT * self.episode_not_dones[i] + td\n",
    "            self.advantages[i] = advantages.detach()               \n",
    "\n",
    "    def stack_tensor(self, some_list):\n",
    "        return torch.cat(some_list[:ROLLOUT_LENGTH], dim=0)\n",
    "            \n",
    "    def get_sample_data(self):\n",
    "        states = self.stack_tensor(self.states)\n",
    "        actions = self.stack_tensor(self.actions) \n",
    "        log_prob_actions = self.stack_tensor(self.log_prob_actions)\n",
    "        returns = self.stack_tensor(self.returns)\n",
    "        # Normalize advantages\n",
    "        advantages = self.stack_tensor(self.advantages)\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()        \n",
    "        return (states, actions, log_prob_actions, returns, advantages)\n",
    "    \n",
    "class MasterAgent():   \n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        self.network = ActorAndCritic(num_agents, state_size, action_size, seed)\n",
    "        self.first_states = True\n",
    "        self.total_steps = 0\n",
    "        self.state_normalizer = MeanStdNormalizer()\n",
    "        \n",
    "    def evaluate_actions_against_states(self, states, actions):\n",
    "        value, action_distribution = self.network(states, actions)\n",
    "        log_prob = self.get_log_prob(action_distribution, actions)\n",
    "        return (log_prob, value)\n",
    "    \n",
    "    def get_log_prob(self, action_distribution, actions):\n",
    "        return action_distribution.log_prob(actions).sum(-1).unsqueeze(-1)\n",
    "    \n",
    "    def get_prediction(self, states):\n",
    "        if self.first_states:\n",
    "            self.states = states\n",
    "            self.first_states = False\n",
    "        #self.latest_actions, self.latest_log_prob, self.latest_values = self.get_prediction_from_states(self.states)\n",
    "        self.latest_values, action_distribution = self.network(self.states)\n",
    "        self.latest_actions = action_distribution.sample()\n",
    "        self.latest_log_prob = self.get_log_prob(action_distribution, self.latest_actions)\n",
    "        return self.latest_actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        rewards = np.asarray(rewards)\n",
    "        next_states = self.state_normalizer(next_states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "        dones = np.asarray(dones).astype(int)\n",
    "        rewards = tensor(rewards).unsqueeze(-1)\n",
    "        episode_not_dones = tensor(1 - dones).unsqueeze(-1)\n",
    "        states = tensor(self.states)        \n",
    "        self.rollout.save_consequences(rewards, episode_not_dones, states)\n",
    "\n",
    "        self.states = next_states\n",
    "                \n",
    "    def start_rollout(self):\n",
    "        self.rollout = Rollout()\n",
    "            \n",
    "    def process_rollout(self, states):\n",
    "        self.save_final_results(states)\n",
    "        self.rollout.calculate_returns_and_advantages(self.latest_values.detach())\n",
    "        self.optimize()\n",
    "        self.first_states = True\n",
    "        \n",
    "    def save_final_results(self, states):    \n",
    "        self.get_prediction(states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "   \n",
    "    def save_weights(self):\n",
    "        print(\"======== Saving weights ==========\")\n",
    "        torch.save(self.network.state_dict(), \"trained_weights.pth\")\n",
    "\n",
    "    def optimize(self):\n",
    "        # Now use tensors for 's', 'a', 'log_pi_a', 'ret', 'adv' for training\n",
    "        # states, actions, log prob actions, returns, advantages (1 row / timestep, 1 column per worker)\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = self.rollout.get_sample_data()\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        optimizer = torch.optim.Adam(self.network.parameters(), 3e-4, eps=1e-5)\n",
    "        for i in range(OPTIMIZATION_EPOCHS):\n",
    "            number_timesteps = states.size(0)\n",
    "            timesteps_to_sample = random_sample(np.arange(number_timesteps), MINI_BATCH_SIZE) \n",
    "            for timestep in timesteps_to_sample:\n",
    "                t = tensor(timestep).long()\n",
    "                # Get data for all workers from sampled timestep \n",
    "                sampled_states = states[t]\n",
    "                sampled_actions = actions[t]\n",
    "                sampled_log_probs_old = log_probs_old[t]\n",
    "                sampled_returns = returns[t]\n",
    "                sampled_advantages = advantages[t]\n",
    "                self.optimize_with_sampled_worker_data(optimizer, sampled_states,\n",
    "                                                                  sampled_actions,\n",
    "                                                                  sampled_log_probs_old,\n",
    "                                                                  sampled_returns,\n",
    "                                                                  sampled_advantages)\n",
    "        steps = ROLLOUT_LENGTH * num_agents\n",
    "        # Total steps used to train network\n",
    "        self.total_steps += steps\n",
    "        \n",
    "    def optimize_with_sampled_worker_data(self, optimizer, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages):\n",
    "        # Get log_prob(actions) and value given states\n",
    "        # Pass in states for all workers x batch_size.\n",
    "        log_prob_action, value = self.evaluate_actions_against_states(sampled_states, sampled_actions)\n",
    "        \n",
    "        policy_loss = self.get_policy_loss(log_prob_action, sampled_log_probs_old, sampled_advantages)\n",
    "        value_loss = self.get_value_loss(value, sampled_returns)\n",
    "        \n",
    "        # Do the actual optimization\n",
    "        optimizer.zero_grad()\n",
    "        # Overall loss function for training both networks at once. Get gradients on weights.\n",
    "        (policy_loss + value_loss).backward()\n",
    "        # Clip weight gradients \n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP) \n",
    "        # Run actual optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    def get_policy_loss(self, log_prob_action, sampled_log_probs_old, sampled_advantages):\n",
    "        # This is the core of PPO\n",
    "        # ratio = new prob / old prob for all workers\n",
    "        ratio = (log_prob_action - sampled_log_probs_old).exp() \n",
    "        # Clip loss on the upside\n",
    "        clamped_ratio = ratio.clamp(1.0 - PPO_RATIO_CLIP, 1.0 + PPO_RATIO_CLIP)\n",
    "        obj = ratio * sampled_advantages\n",
    "        obj_clipped = clamped_ratio * sampled_advantages\n",
    "        policy_loss = -torch.min(obj, obj_clipped).mean() \n",
    "        return policy_loss\n",
    "    \n",
    "    def get_value_loss(self, value, sampled_returns):\n",
    "        # Mean squared error\n",
    "        value_loss = 0.5 * (sampled_returns - value).pow(2).mean()\n",
    "        return value_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The TrainingSession class trains the agent while monitoring the progress of the episodes. It can also simply run an episode with the previously trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSession():\n",
    "    \n",
    "    def __init__(self, num_workers):\n",
    "        self.num_workers = num_workers\n",
    "        self.online_rewards = np.zeros(num_workers)\n",
    "        self.mean_last_100 = 0\n",
    "        self.mean_scores = []\n",
    "\n",
    "    def train_ppo(self, agent, target_average_score, max_episodes=300):\n",
    "        print(\"Attempting to reach 100 episode trailing average of {:.2f} in under {} episodes.\".format(target_average_score, max_episodes))\n",
    "        print(\"Rollout length: %s\" % ROLLOUT_LENGTH)\n",
    "        print(\"GRADIENT_CLIP %s\" % GRADIENT_CLIP)\n",
    "        print(\"PPO_RATIO_CLIP %s\" % PPO_RATIO_CLIP)\n",
    "        print(\"GAE_LAMBDA %s\" % GAE_LAMBDA)\n",
    "        #agent.save_weights()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment   \n",
    "        self.num_episodes = 0\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.episode_scores = [list() for i in range(num_agents)]\n",
    "        self.episodes_finished = 0\n",
    "        self.target_average_score = target_average_score\n",
    "        while True:\n",
    "            mean_last_100 = self.run_rollout(agent, env_info)\n",
    "            agent.process_rollout(states)\n",
    "            \n",
    "            #print(\"\\rEpisode {}\\tLast 100: {:.2f}\".format(self.num_episodes, mean_last_100))\n",
    "            if mean_last_100 > target_average_score:\n",
    "                print(\"Reached target! mean_last_100 %s\" % mean_last_100)\n",
    "                agent.save_weights() \n",
    "                break\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Failed to reach target in {} episodes.\".format(self.num_episodes))\n",
    "                break\n",
    "        return self.mean_scores \n",
    "    \n",
    "    def get_actions_from_policy(self, states):\n",
    "        actions = agent.get_prediction(states)          # Run the policy \n",
    "        actions = to_np(actions)                        # Extract actions\n",
    "        actions = np.clip(actions, -1, 1)               # all actions between -1 and 1\n",
    "        return actions\n",
    "    \n",
    "    def run_ppo(self, agent, max_episodes=5):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.num_episodes = 0\n",
    "        while True:\n",
    "            mean_score_over_agents = self.run_rollout(agent, env_info)\n",
    "            self.num_episodes += 1\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Policy failed to reach target in %s\" % max_episodes)\n",
    "                break\n",
    "    \n",
    "    def step_environment(self, env, actions):\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment        \n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished for each agent\n",
    "        return (next_states, rewards, dones)\n",
    "                                \n",
    "    def run_episode(self, env_info):\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        while True:\n",
    "            actions = self.get_actions_from_policy(states)\n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "            \n",
    "    def run_rollout(self, agent, env_info):\n",
    "        #print(\"Run rollout\")\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        for t in range(ROLLOUT_LENGTH):\n",
    "            actions = self.get_actions_from_policy(states)            \n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "        \n",
    "            # Save rewards\n",
    "            self.online_rewards += rewards                          # Accumulate ongoing (un-normalized) rewards for each agent\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:                                       # For a worker whose episode is done...\n",
    "                    #print(\"Worker %s finished at timestep %s\" % (i, t))\n",
    "                    self.end_episode(i, self.online_rewards[i])\n",
    "                    self.online_rewards[i] = 0                 # Reset accumulated reward for next episode\n",
    "                    self.mean_last_100 = np.mean(self.last_100_scores)\n",
    "                    if self.mean_last_100 > self.target_average_score:\n",
    "                        print(\"Breaking\")\n",
    "                        break\n",
    "            #print(\"%s step\" % t)            \n",
    "            agent.step(states, actions, rewards, next_states, dones) # Teach the agent \n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "\n",
    "        #print(\"Steps in rollout: {}\".format(t+1))        \n",
    "        return self.mean_last_100    \n",
    "\n",
    "    def end_episode(self, agent_index, score):\n",
    "        self.episode_scores[agent_index].append(score)   # Save the reward they accumulated in the episode\n",
    "        self.episodes_finished +=1\n",
    "        if (self.episodes_finished % num_agents) == 0:\n",
    "            self.num_episodes += 1\n",
    "            total_over_agents = 0\n",
    "            for i in range(num_agents):\n",
    "                total_over_agents += self.episode_scores[i][-1]\n",
    "            mean_score_over_agents = total_over_agents / num_agents    \n",
    "            self.last_100_scores.append(mean_score_over_agents)\n",
    "            self.mean_scores.append(mean_score_over_agents)\n",
    "            print(\"Finished %s episodes (%s cycles). mean_score_over_agents %s trailing %s\" % (self.num_episodes, (self.episodes_finished/num_agents), mean_score_over_agents, np.mean(self.last_100_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Policy\n",
    "\n",
    "Running the code in the cell below trains the policy, attempting to reach the target treailing avewrage value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reach 100 episode trailing average of 0.20 in under 300 episodes.\n",
      "Rollout length: 250\n",
      "GRADIENT_CLIP 0.75\n",
      "PPO_RATIO_CLIP 0.1\n",
      "GAE_LAMBDA 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\drlnd2\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\ProgramData\\anaconda3\\envs\\drlnd2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\anaconda3\\envs\\drlnd2\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 episodes (1.0 cycles). mean_score_over_agents 0.24249999457970262 trailing 0.24249999457970262\n",
      "Breaking\n",
      "Reached target! mean_last_100 0.24249999457970262\n",
      "======== Saving weights ==========\n",
      "Training run time：8.028068s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = MasterAgent(num_agents, state_size=state_size, action_size=action_size, seed=0)\n",
    "import time\n",
    "session = TrainingSession(num_agents)\n",
    "start_time = time.time()\n",
    "scores = session.train_ppo(agent, 0.20)   # Do the training\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training run time：{:.6f}s\".format(run_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Policy\n",
    "The code below runs the policy that has previously been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\drlnd2\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\ProgramData\\anaconda3\\envs\\drlnd2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\anaconda3\\envs\\drlnd2\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 5 episodes (149.0 cycles). mean_score_over_agents 37.74299915637821 trailing 37.74299915637821\n",
      "Breaking\n",
      "Policy failed to reach target in 5\n"
     ]
    }
   ],
   "source": [
    "session.run_ppo(agent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd2",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
