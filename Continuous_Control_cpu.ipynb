{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Windows_x86_64/Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "True\n",
      "1.10.0+cu113\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "#DEVICE = torch.device('cuda:0')\n",
    "ROLLOUT_LENGTH = 250 \n",
    "DISCOUNT = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "OPTIMIZATION_EPOCHS = 10\n",
    "MINI_BATCH_SIZE = 64\n",
    "PPO_RATIO_CLIP = 0.1\n",
    "GRADIENT_CLIP = 0.75\n",
    "HIDDEN_LAYERS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-party Code\n",
    "These are helper routines copied or adapted from other projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to Shangtong Zhang \n",
    "# https://github.com/ShangtongZhang/DeepRL\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "def to_np(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "def tensor(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = torch.tensor(x, device=DEVICE, dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]\n",
    "\n",
    "class MeanStdNormalizer():\n",
    "    def __init__(self):\n",
    "        self.rms = None\n",
    "        self.clip = 10.0\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.rms is None:\n",
    "            self.rms = RunningMeanStd(shape=(1, ) + x.shape[1:])\n",
    "        self.rms.update(x)\n",
    "        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n",
    "                       -self.clip, self.clip)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to OpenAI.\n",
    "\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/running_mean_std.py    \n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)    \n",
    "        \n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Agent\n",
    "\n",
    "The master agent implements the PPO algorithm and can use multiple sub-agents for the purpose of samlping trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class SubNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, output_size, seed):\n",
    "        super(SubNetwork, self).__init__()\n",
    "        dims = (input_size,) + hidden_units        \n",
    "        self.layers = nn.ModuleList([layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.feature_dim = dims[-1]\n",
    "        self.output_layer = layer_init(nn.Linear(self.feature_dim, output_size), 1e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.tanh(layer(x))\n",
    "        x = self.output_layer(x)    \n",
    "        return x    \n",
    "            \n",
    "class ActorAndCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        super(ActorAndCritic, self).__init__()\n",
    "        self.seed = random.seed(seed)\n",
    "        self.actor = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), action_size, seed)\n",
    "        self.critic = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), 1, seed)\n",
    "        self.std = nn.Parameter(torch.zeros(action_size))\n",
    "        #self.to(Config.DEVICE)\n",
    "        \n",
    "    def forward(self, obs, action=None):\n",
    "        obs = tensor(obs)\n",
    "        a = self.actor(obs)\n",
    "        v = self.critic(obs)\n",
    "        mean = F.tanh(a)\n",
    "        dist = torch.distributions.Normal(mean, F.softplus(self.std))\n",
    "        return (v, dist)\n",
    "        \n",
    "class Rollout():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stored values\n",
    "        self.actions = []\n",
    "        self.log_prob_actions = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.episode_not_dones = []\n",
    "        self.states = []\n",
    "        # Calculated values\n",
    "        self.returns = [0.0] * ROLLOUT_LENGTH\n",
    "        self.advantages = [0.0] * ROLLOUT_LENGTH\n",
    "        \n",
    "    def save_prediction(self, actions, log_prob_actions, values):\n",
    "        self.actions.append(actions)\n",
    "        self.log_prob_actions.append(log_prob_actions)\n",
    "        self.values.append(values)\n",
    "\n",
    "    def save_consequences(self, rewards, episode_not_dones, states):\n",
    "        self.rewards.append(rewards)\n",
    "        self.episode_not_dones.append(episode_not_dones)\n",
    "        self.states.append(states)\n",
    "        \n",
    "    def calculate_returns_and_advantages(self, final_reward):\n",
    "        self.rewards.append(None)\n",
    "        self.episode_not_dones.append(None)\n",
    "        self.calculate_future_returns(final_reward)\n",
    "        self.estimate_advantages()\n",
    "\n",
    "    def calculate_future_returns(self, returns):\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            returns = self.rewards[i] + DISCOUNT * self.episode_not_dones[i] * returns\n",
    "            self.returns[i] = returns.detach() \n",
    "\n",
    "    def estimate_advantages(self):\n",
    "        advantages = tensor(np.zeros((num_agents, 1)))\n",
    "        # Go backwards through rollout steps and calculate advantages for each state action pair\n",
    "        # Use GAE for PPO. (Schulman, Moritz, Levine et al. 2016)\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            td = self.rewards[i] + (DISCOUNT * self.episode_not_dones[i] * self.values[i + 1]) - self.values[i]\n",
    "            advantages = advantages * GAE_LAMBDA * DISCOUNT * self.episode_not_dones[i] + td\n",
    "            self.advantages[i] = advantages.detach()               \n",
    "\n",
    "    def stack_tensor(self, some_list):\n",
    "        return torch.cat(some_list[:ROLLOUT_LENGTH], dim=0)\n",
    "            \n",
    "    def get_sample_data(self):\n",
    "        states = self.stack_tensor(self.states)\n",
    "        actions = self.stack_tensor(self.actions) \n",
    "        log_prob_actions = self.stack_tensor(self.log_prob_actions)\n",
    "        returns = self.stack_tensor(self.returns)\n",
    "        # Normalize advantages\n",
    "        advantages = self.stack_tensor(self.advantages)\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()        \n",
    "        return (states, actions, log_prob_actions, returns, advantages)\n",
    "    \n",
    "class MasterAgent():   \n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        self.network = ActorAndCritic(num_agents, state_size, action_size, seed)\n",
    "        self.first_states = True\n",
    "        self.total_steps = 0\n",
    "        self.state_normalizer = MeanStdNormalizer()\n",
    "        \n",
    "    def evaluate_actions_against_states(self, states, actions):\n",
    "        value, action_distribution = self.network(states, actions)\n",
    "        log_prob = self.get_log_prob(action_distribution, actions)\n",
    "        return (log_prob, value)\n",
    "    \n",
    "    def get_log_prob(self, action_distribution, actions):\n",
    "        return action_distribution.log_prob(actions).sum(-1).unsqueeze(-1)\n",
    "    \n",
    "    def get_prediction(self, states):\n",
    "        if self.first_states:\n",
    "            self.states = states\n",
    "            self.first_states = False\n",
    "        #self.latest_actions, self.latest_log_prob, self.latest_values = self.get_prediction_from_states(self.states)\n",
    "        self.latest_values, action_distribution = self.network(self.states)\n",
    "        self.latest_actions = action_distribution.sample()\n",
    "        self.latest_log_prob = self.get_log_prob(action_distribution, self.latest_actions)\n",
    "        return self.latest_actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        rewards = np.asarray(rewards)\n",
    "        next_states = self.state_normalizer(next_states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "        dones = np.asarray(dones).astype(int)\n",
    "        rewards = tensor(rewards).unsqueeze(-1)\n",
    "        episode_not_dones = tensor(1 - dones).unsqueeze(-1)\n",
    "        states = tensor(self.states)        \n",
    "        self.rollout.save_consequences(rewards, episode_not_dones, states)\n",
    "\n",
    "        self.states = next_states\n",
    "                \n",
    "    def start_rollout(self):\n",
    "        self.rollout = Rollout()\n",
    "            \n",
    "    def process_rollout(self, states):\n",
    "        self.save_final_results(states)\n",
    "        self.rollout.calculate_returns_and_advantages(self.latest_values.detach())\n",
    "        self.optimize()\n",
    "        self.first_states = True\n",
    "        \n",
    "    def save_final_results(self, states):    \n",
    "        self.get_prediction(states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "   \n",
    "    def save_weights(self):\n",
    "        print(\"======== Saving weights ==========\")\n",
    "        torch.save(self.network.state_dict(), \"trained_weights.pth\")\n",
    "\n",
    "    def optimize(self):\n",
    "        # Now use tensors for 's', 'a', 'log_pi_a', 'ret', 'adv' for training\n",
    "        # states, actions, log prob actions, returns, advantages (1 row / timestep, 1 column per worker)\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = self.rollout.get_sample_data()\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        optimizer = torch.optim.Adam(self.network.parameters(), 3e-4, eps=1e-5)\n",
    "        for i in range(OPTIMIZATION_EPOCHS):\n",
    "            number_timesteps = states.size(0)\n",
    "            timesteps_to_sample = random_sample(np.arange(number_timesteps), MINI_BATCH_SIZE) \n",
    "            for timestep in timesteps_to_sample:\n",
    "                t = tensor(timestep).long()\n",
    "                # Get data for all workers from sampled timestep \n",
    "                sampled_states = states[t]\n",
    "                sampled_actions = actions[t]\n",
    "                sampled_log_probs_old = log_probs_old[t]\n",
    "                sampled_returns = returns[t]\n",
    "                sampled_advantages = advantages[t]\n",
    "                self.optimize_with_sampled_worker_data(optimizer, sampled_states,\n",
    "                                                                  sampled_actions,\n",
    "                                                                  sampled_log_probs_old,\n",
    "                                                                  sampled_returns,\n",
    "                                                                  sampled_advantages)\n",
    "        steps = ROLLOUT_LENGTH * num_agents\n",
    "        # Total steps used to train network\n",
    "        self.total_steps += steps\n",
    "        \n",
    "    def optimize_with_sampled_worker_data(self, optimizer, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages):\n",
    "        # Get log_prob(actions) and value given states\n",
    "        # Pass in states for all workers x batch_size.\n",
    "        log_prob_action, value = self.evaluate_actions_against_states(sampled_states, sampled_actions)\n",
    "        \n",
    "        policy_loss = self.get_policy_loss(log_prob_action, sampled_log_probs_old, sampled_advantages)\n",
    "        value_loss = self.get_value_loss(value, sampled_returns)\n",
    "        \n",
    "        # Do the actual optimization\n",
    "        optimizer.zero_grad()\n",
    "        # Overall loss function for training both networks at once. Get gradients on weights.\n",
    "        (policy_loss + value_loss).backward()\n",
    "        # Clip weight gradients \n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP) \n",
    "        # Run actual optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    def get_policy_loss(self, log_prob_action, sampled_log_probs_old, sampled_advantages):\n",
    "        # This is the core of PPO\n",
    "        # ratio = new prob / old prob for all workers\n",
    "        ratio = (log_prob_action - sampled_log_probs_old).exp() \n",
    "        # Clip loss on the upside\n",
    "        clamped_ratio = ratio.clamp(1.0 - PPO_RATIO_CLIP, 1.0 + PPO_RATIO_CLIP)\n",
    "        obj = ratio * sampled_advantages\n",
    "        obj_clipped = clamped_ratio * sampled_advantages\n",
    "        policy_loss = -torch.min(obj, obj_clipped).mean() \n",
    "        return policy_loss\n",
    "    \n",
    "    def get_value_loss(self, value, sampled_returns):\n",
    "        # Mean squared error\n",
    "        value_loss = 0.5 * (sampled_returns - value).pow(2).mean()\n",
    "        return value_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The TrainingSession class trains the agent while monitoring the progress of the episodes. It can also simply run an episode with the previously trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSession():\n",
    "    \n",
    "    def __init__(self, num_workers):\n",
    "        self.num_workers = num_workers\n",
    "        self.online_rewards = np.zeros(num_workers)\n",
    "        self.mean_last_100 = 0\n",
    "        self.mean_scores = []\n",
    "\n",
    "    def train_ppo(self, agent, target_average_score, max_episodes=300):\n",
    "        print(\"Attempting to reach 100 episode trailing average of {:.2f} in under {} episodes.\".format(target_average_score, max_episodes))\n",
    "        print(\"Rollout length: %s\" % ROLLOUT_LENGTH)\n",
    "        print(\"GRADIENT_CLIP %s\" % GRADIENT_CLIP)\n",
    "        print(\"PPO_RATIO_CLIP %s\" % PPO_RATIO_CLIP)\n",
    "        print(\"GAE_LAMBDA %s\" % GAE_LAMBDA)\n",
    "        #agent.save_weights()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment   \n",
    "        self.num_episodes = 0\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.episode_scores = [list() for i in range(num_agents)]\n",
    "        self.episodes_finished = 0\n",
    "        self.target_average_score = target_average_score\n",
    "        while True:\n",
    "            mean_last_100 = self.run_rollout(agent, env_info)\n",
    "            agent.process_rollout(states)\n",
    "            \n",
    "            #print(\"\\rEpisode {}\\tLast 100: {:.2f}\".format(self.num_episodes, mean_last_100))\n",
    "            if mean_last_100 > target_average_score:\n",
    "                print(\"Reached target! mean_last_100 %s\" % mean_last_100)\n",
    "                agent.save_weights() \n",
    "                break\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Failed to reach target in {} episodes.\".format(self.num_episodes))\n",
    "                break\n",
    "        return self.mean_scores \n",
    "    \n",
    "    def get_actions_from_policy(self, states):\n",
    "        actions = agent.get_prediction(states)          # Run the policy \n",
    "        actions = to_np(actions)                        # Extract actions\n",
    "        actions = np.clip(actions, -1, 1)               # all actions between -1 and 1\n",
    "        return actions\n",
    "    \n",
    "    def run_ppo(self, agent, max_episodes=5):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.num_episodes = 0\n",
    "        while True:\n",
    "            mean_score_over_agents = self.run_rollout(agent, env_info)\n",
    "            self.num_episodes += 1\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Policy failed to reach target in %s\" % max_episodes)\n",
    "                break\n",
    "    \n",
    "    def step_environment(self, env, actions):\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment        \n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished for each agent\n",
    "        return (next_states, rewards, dones)\n",
    "                                \n",
    "    def run_episode(self, env_info):\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        while True:\n",
    "            actions = self.get_actions_from_policy(states)\n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "            \n",
    "    def run_rollout(self, agent, env_info):\n",
    "        #print(\"Run rollout\")\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        for t in range(ROLLOUT_LENGTH):\n",
    "            actions = self.get_actions_from_policy(states)            \n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "        \n",
    "            # Save rewards\n",
    "            self.online_rewards += rewards                          # Accumulate ongoing (un-normalized) rewards for each agent\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:                                       # For a worker whose episode is done...\n",
    "                    #print(\"Worker %s finished at timestep %s\" % (i, t))\n",
    "                    self.end_episode(i, self.online_rewards[i])\n",
    "                    self.online_rewards[i] = 0                 # Reset accumulated reward for next episode\n",
    "                    self.mean_last_100 = np.mean(self.last_100_scores)\n",
    "                    if self.mean_last_100 > self.target_average_score:\n",
    "                        print(\"Breaking\")\n",
    "                        break\n",
    "            #print(\"%s step\" % t)            \n",
    "            agent.step(states, actions, rewards, next_states, dones) # Teach the agent \n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "\n",
    "        #print(\"Steps in rollout: {}\".format(t+1))        \n",
    "        return self.mean_last_100    \n",
    "\n",
    "    def end_episode(self, agent_index, score):\n",
    "        self.episode_scores[agent_index].append(score)   # Save the reward they accumulated in the episode\n",
    "        self.episodes_finished +=1\n",
    "        if (self.episodes_finished % num_agents) == 0:\n",
    "            self.num_episodes += 1\n",
    "            total_over_agents = 0\n",
    "            for i in range(num_agents):\n",
    "                total_over_agents += self.episode_scores[i][-1]\n",
    "            mean_score_over_agents = total_over_agents / num_agents    \n",
    "            self.last_100_scores.append(mean_score_over_agents)\n",
    "            self.mean_scores.append(mean_score_over_agents)\n",
    "            print(\"Finished %s episodes (%s cycles). mean_score_over_agents %s trailing %s\" % (self.num_episodes, (self.episodes_finished/num_agents), mean_score_over_agents, np.mean(self.last_100_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Policy\n",
    "\n",
    "Running the code in the cell below trains the policy, attempting to reach the target treailing avewrage value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reach 100 episode trailing average of 30.00 in under 300 episodes.\n",
      "Rollout length: 250\n",
      "GRADIENT_CLIP 0.75\n",
      "PPO_RATIO_CLIP 0.1\n",
      "GAE_LAMBDA 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\drlnd4\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\ProgramData\\anaconda3\\envs\\drlnd4\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\anaconda3\\envs\\drlnd4\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 episodes (1.0 cycles). mean_score_over_agents 0.22649999493733047 trailing 0.22649999493733047\n",
      "Finished 2 episodes (2.0 cycles). mean_score_over_agents 0.3439999923110008 trailing 0.28524999362416564\n",
      "Finished 3 episodes (3.0 cycles). mean_score_over_agents 0.5589999875053764 trailing 0.3764999915845692\n",
      "Finished 4 episodes (4.0 cycles). mean_score_over_agents 0.7514999832026661 trailing 0.4702499894890934\n",
      "Finished 5 episodes (5.0 cycles). mean_score_over_agents 0.6059999864548444 trailing 0.4973999888822436\n",
      "Finished 6 episodes (6.0 cycles). mean_score_over_agents 1.2564999719150365 trailing 0.6239166527210425\n",
      "Finished 7 episodes (7.0 cycles). mean_score_over_agents 1.553499965276569 trailing 0.7567142688004035\n",
      "Finished 8 episodes (8.0 cycles). mean_score_over_agents 1.5159999661147594 trailing 0.851624980964698\n",
      "Finished 9 episodes (9.0 cycles). mean_score_over_agents 1.4899999666959047 trailing 0.9225555349348321\n",
      "Finished 10 episodes (10.0 cycles). mean_score_over_agents 2.0459999542683365 trailing 1.0348999768681826\n",
      "Finished 11 episodes (11.0 cycles). mean_score_over_agents 2.125499952491373 trailing 1.1340454291975635\n",
      "Finished 12 episodes (12.0 cycles). mean_score_over_agents 2.2199999503791332 trailing 1.2245416392960276\n",
      "Finished 13 episodes (13.0 cycles). mean_score_over_agents 1.9144999572075903 trailing 1.2776153560584556\n",
      "Finished 14 episodes (14.0 cycles). mean_score_over_agents 2.862499936018139 trailing 1.3908213974841472\n",
      "Finished 15 episodes (15.0 cycles). mean_score_over_agents 3.0314999322406946 trailing 1.500199966467917\n",
      "Finished 16 episodes (16.0 cycles). mean_score_over_agents 2.8899999354034662 trailing 1.5870624645263887\n",
      "Finished 17 episodes (17.0 cycles). mean_score_over_agents 3.3714999246411024 trailing 1.6920293739449013\n",
      "Finished 18 episodes (18.0 cycles). mean_score_over_agents 3.400499923992902 trailing 1.7869444045031235\n",
      "Finished 19 episodes (19.0 cycles). mean_score_over_agents 3.57949991999194 trailing 1.881289431634114\n",
      "Finished 20 episodes (20.0 cycles). mean_score_over_agents 4.984499888587743 trailing 2.036449954481795\n",
      "Finished 21 episodes (21.0 cycles). mean_score_over_agents 4.19699990618974 trailing 2.139333285515507\n",
      "Finished 22 episodes (22.0 cycles). mean_score_over_agents 4.332499903161079 trailing 2.2390226772266693\n",
      "Finished 23 episodes (23.0 cycles). mean_score_over_agents 4.318999903462827 trailing 2.3294564696717193\n",
      "Finished 24 episodes (24.0 cycles). mean_score_over_agents 3.831499914359301 trailing 2.392041613200369\n",
      "Finished 25 episodes (25.0 cycles). mean_score_over_agents 5.119499885570258 trailing 2.5011399440951645\n",
      "Finished 26 episodes (26.0 cycles). mean_score_over_agents 4.689499895181507 trailing 2.585307634521562\n",
      "Finished 27 episodes (27.0 cycles). mean_score_over_agents 5.70099987257272 trailing 2.700703643338272\n",
      "Finished 28 episodes (28.0 cycles). mean_score_over_agents 6.78599984832108 trailing 2.846607079230515\n",
      "Finished 29 episodes (29.0 cycles). mean_score_over_agents 6.708999850042164 trailing 2.97979303684471\n",
      "Finished 30 episodes (30.0 cycles). mean_score_over_agents 6.753499849047512 trailing 3.1055832639181364\n",
      "Finished 31 episodes (31.0 cycles). mean_score_over_agents 5.8499998692423105 trailing 3.1941128318318195\n",
      "Finished 32 episodes (32.0 cycles). mean_score_over_agents 6.680499850679189 trailing 3.3030624261707997\n",
      "Finished 33 episodes (33.0 cycles). mean_score_over_agents 6.636499851662665 trailing 3.404075681488735\n",
      "Finished 34 episodes (34.0 cycles). mean_score_over_agents 6.80899984780699 trailing 3.5042205099098602\n",
      "Finished 35 episodes (35.0 cycles). mean_score_over_agents 7.231499838363379 trailing 3.610714205008532\n",
      "Finished 36 episodes (36.0 cycles). mean_score_over_agents 8.21249981643632 trailing 3.7385415831037485\n",
      "Finished 37 episodes (37.0 cycles). mean_score_over_agents 9.606999785266817 trailing 3.8971485615405883\n",
      "Finished 38 episodes (38.0 cycles). mean_score_over_agents 10.759999759495258 trailing 4.077749908855185\n",
      "Finished 39 episodes (39.0 cycles). mean_score_over_agents 10.175499772559851 trailing 4.2341024694629965\n",
      "Finished 40 episodes (40.0 cycles). mean_score_over_agents 10.97199975475669 trailing 4.402549901595339\n",
      "Finished 41 episodes (41.0 cycles). mean_score_over_agents 11.82099973578006 trailing 4.583487702429113\n",
      "Finished 42 episodes (42.0 cycles). mean_score_over_agents 12.607499718200415 trailing 4.774535607566524\n",
      "Finished 43 episodes (43.0 cycles). mean_score_over_agents 13.139499706309289 trailing 4.969069656374496\n",
      "Finished 44 episodes (44.0 cycles). mean_score_over_agents 13.766999692283571 trailing 5.169022611736066\n",
      "Finished 45 episodes (45.0 cycles). mean_score_over_agents 13.676999694295228 trailing 5.35808876912627\n",
      "Finished 46 episodes (46.0 cycles). mean_score_over_agents 13.659499694686383 trailing 5.538554224029751\n",
      "Finished 47 episodes (47.0 cycles). mean_score_over_agents 14.908999666757882 trailing 5.737925403662263\n",
      "Finished 48 episodes (48.0 cycles). mean_score_over_agents 16.31549963532016 trailing 5.95829153348847\n",
      "Finished 49 episodes (49.0 cycles). mean_score_over_agents 16.818999624066056 trailing 6.179938637377809\n",
      "Finished 50 episodes (50.0 cycles). mean_score_over_agents 19.06949957376346 trailing 6.437729856105522\n",
      "Finished 51 episodes (51.0 cycles). mean_score_over_agents 19.215499570500107 trailing 6.688274360309338\n",
      "Finished 52 episodes (52.0 cycles). mean_score_over_agents 20.90399953275919 trailing 6.961653690548758\n",
      "Finished 53 episodes (53.0 cycles). mean_score_over_agents 19.96899955365807 trailing 7.207075310607424\n",
      "Finished 54 episodes (54.0 cycles). mean_score_over_agents 22.12349950550124 trailing 7.483305388290643\n",
      "Finished 55 episodes (55.0 cycles). mean_score_over_agents 23.079499484132974 trailing 7.766872553669594\n",
      "Finished 56 episodes (56.0 cycles). mean_score_over_agents 22.305999501422047 trailing 8.026499820593745\n",
      "Finished 57 episodes (57.0 cycles). mean_score_over_agents 24.671499448549003 trailing 8.318517357926293\n",
      "Finished 58 episodes (58.0 cycles). mean_score_over_agents 23.133999482914806 trailing 8.573956704908854\n",
      "Finished 59 episodes (59.0 cycles). mean_score_over_agents 21.23949952526018 trailing 8.788626922202944\n",
      "Finished 60 episodes (60.0 cycles). mean_score_over_agents 24.04399946257472 trailing 9.042883131209141\n",
      "Finished 61 episodes (61.0 cycles). mean_score_over_agents 23.665499471034856 trailing 9.282598153173497\n",
      "Finished 62 episodes (62.0 cycles). mean_score_over_agents 23.993499463703483 trailing 9.519870754956239\n",
      "Finished 63 episodes (63.0 cycles). mean_score_over_agents 23.827499467413872 trailing 9.746975972614297\n",
      "Finished 64 episodes (64.0 cycles). mean_score_over_agents 25.892499421257526 trailing 9.999249776499346\n",
      "Finished 65 episodes (65.0 cycles). mean_score_over_agents 27.20899939183146 trailing 10.264015155196763\n",
      "Finished 66 episodes (66.0 cycles). mean_score_over_agents 28.794999356381595 trailing 10.544787643093501\n",
      "Finished 67 episodes (67.0 cycles). mean_score_over_agents 28.06399937272072 trailing 10.80626841517749\n",
      "Finished 68 episodes (68.0 cycles). mean_score_over_agents 26.531999406963585 trailing 11.037529165056696\n",
      "Finished 69 episodes (69.0 cycles). mean_score_over_agents 28.669499359186737 trailing 11.293064964971624\n",
      "Finished 70 episodes (70.0 cycles). mean_score_over_agents 29.551499339472503 trailing 11.553899741750207\n",
      "Finished 71 episodes (71.0 cycles). mean_score_over_agents 30.64799931496382 trailing 11.822830721654626\n",
      "Finished 72 episodes (72.0 cycles). mean_score_over_agents 30.064499328006058 trailing 12.076187230076174\n",
      "Finished 73 episodes (73.0 cycles). mean_score_over_agents 29.73649933533743 trailing 12.31810931370989\n",
      "Finished 74 episodes (74.0 cycles). mean_score_over_agents 30.449499319400637 trailing 12.563128097570575\n",
      "Finished 75 episodes (75.0 cycles). mean_score_over_agents 30.09249932738021 trailing 12.79685304730137\n",
      "Finished 76 episodes (76.0 cycles). mean_score_over_agents 31.485499296244235 trailing 13.042756287419039\n",
      "Finished 77 episodes (77.0 cycles). mean_score_over_agents 29.174499347899108 trailing 13.25225944404865\n",
      "Finished 78 episodes (78.0 cycles). mean_score_over_agents 31.291999300569294 trailing 13.48353816015789\n",
      "Finished 79 episodes (79.0 cycles). mean_score_over_agents 30.48349931864068 trailing 13.698727541910836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 80 episodes (80.0 cycles). mean_score_over_agents 33.12649925956502 trailing 13.941574688381516\n",
      "Finished 81 episodes (81.0 cycles). mean_score_over_agents 30.123499326687305 trailing 14.141351535768008\n",
      "Finished 82 episodes (82.0 cycles). mean_score_over_agents 33.1819992583245 trailing 14.373554556774796\n",
      "Finished 83 episodes (83.0 cycles). mean_score_over_agents 30.399499320518224 trailing 14.566638228627127\n",
      "Finished 84 episodes (84.0 cycles). mean_score_over_agents 31.534499295148997 trailing 14.768636574657151\n",
      "Finished 85 episodes (85.0 cycles). mean_score_over_agents 33.11599925979972 trailing 14.98448790036471\n",
      "Finished 86 episodes (86.0 cycles). mean_score_over_agents 31.715499291103335 trailing 15.179034544443066\n",
      "Finished 87 episodes (87.0 cycles). mean_score_over_agents 30.926499308738858 trailing 15.360039886561408\n",
      "Finished 88 episodes (88.0 cycles). mean_score_over_agents 31.67999929189682 trailing 15.545493970712942\n",
      "Finished 89 episodes (89.0 cycles). mean_score_over_agents 32.19099928047508 trailing 15.73252212026083\n",
      "Finished 90 episodes (90.0 cycles). mean_score_over_agents 32.248999279178676 trailing 15.916038533137696\n",
      "Finished 91 episodes (91.0 cycles). mean_score_over_agents 32.855999265611175 trailing 16.102191947780263\n",
      "Finished 92 episodes (92.0 cycles). mean_score_over_agents 33.01449926206842 trailing 16.28602137510948\n",
      "Finished 93 episodes (93.0 cycles). mean_score_over_agents 32.98099926281721 trailing 16.465537266375158\n",
      "Finished 94 episodes (94.0 cycles). mean_score_over_agents 33.07399926073849 trailing 16.64222303227264\n",
      "Finished 95 episodes (95.0 cycles). mean_score_over_agents 32.67999926954508 trailing 16.811041729507085\n",
      "Finished 96 episodes (96.0 cycles). mean_score_over_agents 32.18849928053096 trailing 16.971223578996916\n",
      "Finished 97 episodes (97.0 cycles). mean_score_over_agents 33.84049924360588 trailing 17.14513363739495\n",
      "Finished 98 episodes (98.0 cycles). mean_score_over_agents 33.62299924846739 trailing 17.313275123222216\n",
      "Finished 99 episodes (99.0 cycles). mean_score_over_agents 33.43649925263598 trailing 17.476135973014273\n",
      "Finished 100 episodes (100.0 cycles). mean_score_over_agents 34.76199922300875 trailing 17.64899460551422\n",
      "Finished 101 episodes (101.0 cycles). mean_score_over_agents 33.98199924044311 trailing 17.98654959796928\n",
      "Finished 102 episodes (102.0 cycles). mean_score_over_agents 33.70149924671277 trailing 18.320124590513295\n",
      "Finished 103 episodes (103.0 cycles). mean_score_over_agents 34.098499237839135 trailing 18.655519583016634\n",
      "Finished 104 episodes (104.0 cycles). mean_score_over_agents 34.70649922424927 trailing 18.995069575427102\n",
      "Finished 105 episodes (105.0 cycles). mean_score_over_agents 34.927999219298364 trailing 19.338289567755535\n",
      "Finished 106 episodes (106.0 cycles). mean_score_over_agents 34.249999234452844 trailing 19.668224560380914\n",
      "Finished 107 episodes (107.0 cycles). mean_score_over_agents 35.124499214906244 trailing 20.00393455287721\n",
      "Finished 108 episodes (108.0 cycles). mean_score_over_agents 33.32149925520643 trailing 20.321989545768126\n",
      "Finished 109 episodes (109.0 cycles). mean_score_over_agents 35.00449921758845 trailing 20.65713453827705\n",
      "Finished 110 episodes (110.0 cycles). mean_score_over_agents 34.812999221868814 trailing 20.984804530953056\n",
      "Finished 111 episodes (111.0 cycles). mean_score_over_agents 35.02049921723083 trailing 21.31375452360045\n",
      "Finished 112 episodes (112.0 cycles). mean_score_over_agents 33.21899925749749 trailing 21.623744516671636\n",
      "Finished 113 episodes (113.0 cycles). mean_score_over_agents 33.60699924882501 trailing 21.94066950958781\n",
      "Finished 114 episodes (114.0 cycles). mean_score_over_agents 35.226499212626365 trailing 22.2643095023539\n",
      "Finished 115 episodes (115.0 cycles). mean_score_over_agents 32.855999265611175 trailing 22.5625544956876\n",
      "Finished 116 episodes (116.0 cycles). mean_score_over_agents 34.77349922275171 trailing 22.881389488561076\n",
      "Finished 117 episodes (117.0 cycles). mean_score_over_agents 34.02049923958257 trailing 23.187879481710493\n",
      "Finished 118 episodes (118.0 cycles). mean_score_over_agents 33.50749925104901 trailing 23.48894947498105\n",
      "Finished 119 episodes (119.0 cycles). mean_score_over_agents 33.30849925549701 trailing 23.786239468336102\n",
      "Finished 120 episodes (120.0 cycles). mean_score_over_agents 33.34599925465882 trailing 24.069854461996812\n",
      "Finished 121 episodes (121.0 cycles). mean_score_over_agents 33.09549926025793 trailing 24.358839455537495\n",
      "Finished 122 episodes (122.0 cycles). mean_score_over_agents 32.77349926745519 trailing 24.64324944918043\n",
      "Finished 123 episodes (123.0 cycles). mean_score_over_agents 33.018999261967835 trailing 24.93024944276549\n",
      "Finished 124 episodes (124.0 cycles). mean_score_over_agents 34.253999234363434 trailing 25.23447443596553\n",
      "Finished 125 episodes (125.0 cycles). mean_score_over_agents 34.0439992390573 trailing 25.523719429500407\n",
      "Finished 126 episodes (126.0 cycles). mean_score_over_agents 35.143999214470384 trailing 25.828264422693294\n",
      "Finished 127 episodes (127.0 cycles). mean_score_over_agents 33.673999247327444 trailing 26.107994416440842\n",
      "Finished 128 episodes (128.0 cycles). mean_score_over_agents 34.277999233826996 trailing 26.382914410295903\n",
      "Finished 129 episodes (129.0 cycles). mean_score_over_agents 35.24249921226874 trailing 26.668249403918168\n",
      "Finished 130 episodes (130.0 cycles). mean_score_over_agents 34.64049922572449 trailing 26.947119397684933\n",
      "Finished 131 episodes (131.0 cycles). mean_score_over_agents 34.060999238677326 trailing 27.229229391379278\n",
      "Finished 132 episodes (132.0 cycles). mean_score_over_agents 34.86349922074005 trailing 27.51105938507989\n",
      "Finished 133 episodes (133.0 cycles). mean_score_over_agents 33.78299924489111 trailing 27.78252437901217\n",
      "Finished 134 episodes (134.0 cycles). mean_score_over_agents 35.19999921321869 trailing 28.066434372666286\n",
      "Finished 135 episodes (135.0 cycles). mean_score_over_agents 34.09549923790619 trailing 28.33507436666172\n",
      "Finished 136 episodes (136.0 cycles). mean_score_over_agents 34.031499239336696 trailing 28.593264360890725\n",
      "Finished 137 episodes (137.0 cycles). mean_score_over_agents 35.82149919932708 trailing 28.85540935503133\n",
      "Finished 138 episodes (138.0 cycles). mean_score_over_agents 32.70149926906451 trailing 29.074824350127024\n",
      "Finished 139 episodes (139.0 cycles). mean_score_over_agents 34.717499224003404 trailing 29.32024434464146\n",
      "Finished 140 episodes (140.0 cycles). mean_score_over_agents 35.020999217219654 trailing 29.56073433926609\n",
      "Finished 141 episodes (141.0 cycles). mean_score_over_agents 35.77499920036644 trailing 29.800274333911947\n",
      "Finished 142 episodes (142.0 cycles). mean_score_over_agents 35.72549920147285 trailing 30.031454328744672\n",
      "Breaking\n",
      "Reached target! mean_last_100 30.031454328744672\n",
      "======== Saving weights ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run time：1121.721874s\n"
     ]
    }
   ],
   "source": [
    "agent = MasterAgent(num_agents, state_size=state_size, action_size=action_size, seed=0)\n",
    "import time\n",
    "session = TrainingSession(num_agents)\n",
    "start_time = time.time()\n",
    "scores = session.train_ppo(agent, 30)   # Do the training\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training run time：{:.6f}s\".format(run_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Policy\n",
    "The code below runs the policy that has previously been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\drlnd4\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\ProgramData\\anaconda3\\envs\\drlnd4\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\anaconda3\\envs\\drlnd4\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 5 episodes (143.0 cycles). mean_score_over_agents 38.302999143861236 trailing 38.302999143861236\n",
      "Breaking\n",
      "Policy failed to reach target in 5\n"
     ]
    }
   ],
   "source": [
    "session.run_ppo(agent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
