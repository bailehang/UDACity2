{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "True\n",
      "1.10.0+cu113\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "#DEVICE = torch.device('cpu')\n",
    "DEVICE = torch.device('cuda:0')\n",
    "ROLLOUT_LENGTH = 250 \n",
    "DISCOUNT = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "OPTIMIZATION_EPOCHS = 10\n",
    "MINI_BATCH_SIZE = 64\n",
    "PPO_RATIO_CLIP = 0.1\n",
    "GRADIENT_CLIP = 0.75\n",
    "HIDDEN_LAYERS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-party Code\n",
    "These are helper routines copied or adapted from other projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to Shangtong Zhang \n",
    "# https://github.com/ShangtongZhang/DeepRL\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "def to_np(t):\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "def tensor(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = torch.tensor(x, device=DEVICE, dtype=torch.float32)\n",
    "    return x\n",
    "\n",
    "def random_sample(indices, batch_size):\n",
    "    indices = np.asarray(np.random.permutation(indices))\n",
    "    batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "    for batch in batches:\n",
    "        yield batch\n",
    "    r = len(indices) % batch_size\n",
    "    if r:\n",
    "        yield indices[-r:]\n",
    "\n",
    "class MeanStdNormalizer():\n",
    "    def __init__(self):\n",
    "        self.rms = None\n",
    "        self.clip = 10.0\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.rms is None:\n",
    "            self.rms = RunningMeanStd(shape=(1, ) + x.shape[1:])\n",
    "        self.rms.update(x)\n",
    "        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n",
    "                       -self.clip, self.clip)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for these helper routines to OpenAI.\n",
    "\n",
    "# https://github.com/openai/baselines/blob/master/baselines/common/running_mean_std.py    \n",
    "class RunningMeanStd(object):\n",
    "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
    "    def __init__(self, epsilon=1e-4, shape=()):\n",
    "        self.mean = np.zeros(shape, 'float64')\n",
    "        self.var = np.ones(shape, 'float64')\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
    "\n",
    "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
    "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
    "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count)    \n",
    "        \n",
    "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
    "    delta = batch_mean - mean\n",
    "    tot_count = count + batch_count\n",
    "\n",
    "    new_mean = mean + delta * batch_count / tot_count\n",
    "    m_a = var * count\n",
    "    m_b = batch_var * batch_count\n",
    "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
    "    new_var = M2 / tot_count\n",
    "    new_count = tot_count\n",
    "\n",
    "    return new_mean, new_var, new_count\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Agent\n",
    "\n",
    "The master agent implements the PPO algorithm and can use multiple sub-agents for the purpose of samlping trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "class SubNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_units, output_size, seed):\n",
    "        super(SubNetwork, self).__init__()\n",
    "        dims = (input_size,) + hidden_units        \n",
    "        self.layers = nn.ModuleList([layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.feature_dim = dims[-1]\n",
    "        self.output_layer = layer_init(nn.Linear(self.feature_dim, output_size), 1e-3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.tanh(layer(x))\n",
    "        x = self.output_layer(x)    \n",
    "        return x    \n",
    "            \n",
    "class ActorAndCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        super(ActorAndCritic, self).__init__()\n",
    "        self.seed = random.seed(seed)\n",
    "        self.actor = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), action_size, seed)\n",
    "        self.critic = SubNetwork(state_size, (HIDDEN_LAYERS, HIDDEN_LAYERS), 1, seed)\n",
    "        self.std = nn.Parameter(torch.zeros(action_size))\n",
    "        self.to(DEVICE)\n",
    "        \n",
    "    def forward(self, obs, action=None):\n",
    "        obs = tensor(obs)\n",
    "        a = self.actor(obs)\n",
    "        v = self.critic(obs)\n",
    "        mean = F.tanh(a)\n",
    "        dist = torch.distributions.Normal(mean, F.softplus(self.std))\n",
    "        return (v, dist)\n",
    "        \n",
    "class Rollout():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stored values\n",
    "        self.actions = []\n",
    "        self.log_prob_actions = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.episode_not_dones = []\n",
    "        self.states = []\n",
    "        # Calculated values\n",
    "        self.returns = [0.0] * ROLLOUT_LENGTH\n",
    "        self.advantages = [0.0] * ROLLOUT_LENGTH\n",
    "        \n",
    "    def save_prediction(self, actions, log_prob_actions, values):\n",
    "        self.actions.append(actions)\n",
    "        self.log_prob_actions.append(log_prob_actions)\n",
    "        self.values.append(values)\n",
    "\n",
    "    def save_consequences(self, rewards, episode_not_dones, states):\n",
    "        self.rewards.append(rewards)\n",
    "        self.episode_not_dones.append(episode_not_dones)\n",
    "        self.states.append(states)\n",
    "        \n",
    "    def calculate_returns_and_advantages(self, final_reward):\n",
    "        self.rewards.append(None)\n",
    "        self.episode_not_dones.append(None)\n",
    "        self.calculate_future_returns(final_reward)\n",
    "        self.estimate_advantages()\n",
    "\n",
    "    def calculate_future_returns(self, returns):\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            returns = self.rewards[i] + DISCOUNT * self.episode_not_dones[i] * returns\n",
    "            self.returns[i] = returns.detach() \n",
    "\n",
    "    def estimate_advantages(self):\n",
    "        advantages = tensor(np.zeros((num_agents, 1)))\n",
    "        # Go backwards through rollout steps and calculate advantages for each state action pair\n",
    "        # Use GAE for PPO. (Schulman, Moritz, Levine et al. 2016)\n",
    "        for i in reversed(range(ROLLOUT_LENGTH)):\n",
    "            td = self.rewards[i] + (DISCOUNT * self.episode_not_dones[i] * self.values[i + 1]) - self.values[i]\n",
    "            advantages = advantages * GAE_LAMBDA * DISCOUNT * self.episode_not_dones[i] + td\n",
    "            self.advantages[i] = advantages.detach()               \n",
    "\n",
    "    def stack_tensor(self, some_list):\n",
    "        return torch.cat(some_list[:ROLLOUT_LENGTH], dim=0)\n",
    "            \n",
    "    def get_sample_data(self):\n",
    "        states = self.stack_tensor(self.states)\n",
    "        actions = self.stack_tensor(self.actions) \n",
    "        log_prob_actions = self.stack_tensor(self.log_prob_actions)\n",
    "        returns = self.stack_tensor(self.returns)\n",
    "        # Normalize advantages\n",
    "        advantages = self.stack_tensor(self.advantages)\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()        \n",
    "        return (states, actions, log_prob_actions, returns, advantages)\n",
    "    \n",
    "class MasterAgent():   \n",
    "    \n",
    "    def __init__(self, num_agents, state_size, action_size, seed):\n",
    "        self.network = ActorAndCritic(num_agents, state_size, action_size, seed)\n",
    "        self.first_states = True\n",
    "        self.total_steps = 0\n",
    "        self.state_normalizer = MeanStdNormalizer()\n",
    "        \n",
    "    def evaluate_actions_against_states(self, states, actions):\n",
    "        value, action_distribution = self.network(states, actions)\n",
    "        log_prob = self.get_log_prob(action_distribution, actions)\n",
    "        return (log_prob, value)\n",
    "    \n",
    "    def get_log_prob(self, action_distribution, actions):\n",
    "        return action_distribution.log_prob(actions).sum(-1).unsqueeze(-1)\n",
    "    \n",
    "    def get_prediction(self, states):\n",
    "        if self.first_states:\n",
    "            self.states = states\n",
    "            self.first_states = False\n",
    "        #self.latest_actions, self.latest_log_prob, self.latest_values = self.get_prediction_from_states(self.states)\n",
    "        self.latest_values, action_distribution = self.network(self.states)\n",
    "        self.latest_actions = action_distribution.sample()\n",
    "        self.latest_log_prob = self.get_log_prob(action_distribution, self.latest_actions)\n",
    "        return self.latest_actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        rewards = np.asarray(rewards)\n",
    "        next_states = self.state_normalizer(next_states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "        dones = np.asarray(dones).astype(int)\n",
    "        rewards = tensor(rewards).unsqueeze(-1)\n",
    "        episode_not_dones = tensor(1 - dones).unsqueeze(-1)\n",
    "        states = tensor(self.states)        \n",
    "        self.rollout.save_consequences(rewards, episode_not_dones, states)\n",
    "\n",
    "        self.states = next_states\n",
    "                \n",
    "    def start_rollout(self):\n",
    "        self.rollout = Rollout()\n",
    "            \n",
    "    def process_rollout(self, states):\n",
    "        self.save_final_results(states)\n",
    "        self.rollout.calculate_returns_and_advantages(self.latest_values.detach())\n",
    "        self.optimize()\n",
    "        self.first_states = True\n",
    "        \n",
    "    def save_final_results(self, states):    \n",
    "        self.get_prediction(states)\n",
    "        self.rollout.save_prediction(self.latest_actions, self.latest_log_prob, self.latest_values)\n",
    "   \n",
    "    def save_weights(self):\n",
    "        print(\"======== Saving weights ==========\")\n",
    "        torch.save(self.network.state_dict(), \"trained_weights.pth\")\n",
    "\n",
    "    def optimize(self):\n",
    "        # Now use tensors for 's', 'a', 'log_pi_a', 'ret', 'adv' for training\n",
    "        # states, actions, log prob actions, returns, advantages (1 row / timestep, 1 column per worker)\n",
    "\n",
    "        states, actions, log_probs_old, returns, advantages = self.rollout.get_sample_data()\n",
    "        actions = actions.detach()\n",
    "        log_probs_old = log_probs_old.detach()\n",
    "        optimizer = torch.optim.Adam(self.network.parameters(), 3e-4, eps=1e-5)\n",
    "        for i in range(OPTIMIZATION_EPOCHS):\n",
    "            number_timesteps = states.size(0)\n",
    "            timesteps_to_sample = random_sample(np.arange(number_timesteps), MINI_BATCH_SIZE) \n",
    "            for timestep in timesteps_to_sample:\n",
    "                t = tensor(timestep).long()\n",
    "                # Get data for all workers from sampled timestep \n",
    "                sampled_states = states[t]\n",
    "                sampled_actions = actions[t]\n",
    "                sampled_log_probs_old = log_probs_old[t]\n",
    "                sampled_returns = returns[t]\n",
    "                sampled_advantages = advantages[t]\n",
    "                self.optimize_with_sampled_worker_data(optimizer, sampled_states,\n",
    "                                                                  sampled_actions,\n",
    "                                                                  sampled_log_probs_old,\n",
    "                                                                  sampled_returns,\n",
    "                                                                  sampled_advantages)\n",
    "        steps = ROLLOUT_LENGTH * num_agents\n",
    "        # Total steps used to train network\n",
    "        self.total_steps += steps\n",
    "        \n",
    "    def optimize_with_sampled_worker_data(self, optimizer, sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages):\n",
    "        # Get log_prob(actions) and value given states\n",
    "        # Pass in states for all workers x batch_size.\n",
    "        log_prob_action, value = self.evaluate_actions_against_states(sampled_states, sampled_actions)\n",
    "        \n",
    "        policy_loss = self.get_policy_loss(log_prob_action, sampled_log_probs_old, sampled_advantages)\n",
    "        value_loss = self.get_value_loss(value, sampled_returns)\n",
    "        \n",
    "        # Do the actual optimization\n",
    "        optimizer.zero_grad()\n",
    "        # Overall loss function for training both networks at once. Get gradients on weights.\n",
    "        (policy_loss + value_loss).backward()\n",
    "        # Clip weight gradients \n",
    "        nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP) \n",
    "        # Run actual optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    def get_policy_loss(self, log_prob_action, sampled_log_probs_old, sampled_advantages):\n",
    "        # This is the core of PPO\n",
    "        # ratio = new prob / old prob for all workers\n",
    "        ratio = (log_prob_action - sampled_log_probs_old).exp() \n",
    "        # Clip loss on the upside\n",
    "        clamped_ratio = ratio.clamp(1.0 - PPO_RATIO_CLIP, 1.0 + PPO_RATIO_CLIP)\n",
    "        obj = ratio * sampled_advantages\n",
    "        obj_clipped = clamped_ratio * sampled_advantages\n",
    "        policy_loss = -torch.min(obj, obj_clipped).mean() \n",
    "        return policy_loss\n",
    "    \n",
    "    def get_value_loss(self, value, sampled_returns):\n",
    "        # Mean squared error\n",
    "        value_loss = 0.5 * (sampled_returns - value).pow(2).mean()\n",
    "        return value_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The TrainingSession class trains the agent while monitoring the progress of the episodes. It can also simply run an episode with the previously trained policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSession():\n",
    "    \n",
    "    def __init__(self, num_workers):\n",
    "        self.num_workers = num_workers\n",
    "        self.online_rewards = np.zeros(num_workers)\n",
    "        self.mean_last_100 = 0\n",
    "        self.mean_scores = []\n",
    "\n",
    "    def train_ppo(self, agent, target_average_score, max_episodes=300):\n",
    "        print(\"Attempting to reach 100 episode trailing average of {:.2f} in under {} episodes.\".format(target_average_score, max_episodes))\n",
    "        print(\"Rollout length: %s\" % ROLLOUT_LENGTH)\n",
    "        print(\"GRADIENT_CLIP %s\" % GRADIENT_CLIP)\n",
    "        print(\"PPO_RATIO_CLIP %s\" % PPO_RATIO_CLIP)\n",
    "        print(\"GAE_LAMBDA %s\" % GAE_LAMBDA)\n",
    "        #agent.save_weights()\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment   \n",
    "        self.num_episodes = 0\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.episode_scores = [list() for i in range(num_agents)]\n",
    "        self.episodes_finished = 0\n",
    "        self.target_average_score = target_average_score\n",
    "        while True:\n",
    "            mean_last_100 = self.run_rollout(agent, env_info)\n",
    "            agent.process_rollout(states)\n",
    "            \n",
    "            #print(\"\\rEpisode {}\\tLast 100: {:.2f}\".format(self.num_episodes, mean_last_100))\n",
    "            if mean_last_100 > target_average_score:\n",
    "                print(\"Reached target! mean_last_100 %s\" % mean_last_100)\n",
    "                agent.save_weights() \n",
    "                break\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Failed to reach target in {} episodes.\".format(self.num_episodes))\n",
    "                break\n",
    "        return self.mean_scores \n",
    "    \n",
    "    def get_actions_from_policy(self, states):\n",
    "        actions = agent.get_prediction(states)          # Run the policy \n",
    "        actions = to_np(actions)                        # Extract actions\n",
    "        actions = np.clip(actions, -1, 1)               # all actions between -1 and 1\n",
    "        return actions\n",
    "    \n",
    "    def run_ppo(self, agent, max_episodes=5):\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        self.last_100_scores = deque(maxlen=100)\n",
    "        self.num_episodes = 0\n",
    "        while True:\n",
    "            mean_score_over_agents = self.run_rollout(agent, env_info)\n",
    "            self.num_episodes += 1\n",
    "            if self.num_episodes > max_episodes:\n",
    "                print(\"Policy failed to reach target in %s\" % max_episodes)\n",
    "                break\n",
    "    \n",
    "    def step_environment(self, env, actions):\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment        \n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished for each agent\n",
    "        return (next_states, rewards, dones)\n",
    "                                \n",
    "    def run_episode(self, env_info):\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        while True:\n",
    "            actions = self.get_actions_from_policy(states)\n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "            \n",
    "    def run_rollout(self, agent, env_info):\n",
    "        #print(\"Run rollout\")\n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        agent.start_rollout()\n",
    "        for t in range(ROLLOUT_LENGTH):\n",
    "            actions = self.get_actions_from_policy(states)            \n",
    "            next_states, rewards, dones = self.step_environment(env, actions)\n",
    "        \n",
    "            # Save rewards\n",
    "            self.online_rewards += rewards                          # Accumulate ongoing (un-normalized) rewards for each agent\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:                                       # For a worker whose episode is done...\n",
    "                    #print(\"Worker %s finished at timestep %s\" % (i, t))\n",
    "                    self.end_episode(i, self.online_rewards[i])\n",
    "                    self.online_rewards[i] = 0                 # Reset accumulated reward for next episode\n",
    "                    self.mean_last_100 = np.mean(self.last_100_scores)\n",
    "                    if self.mean_last_100 > self.target_average_score:\n",
    "                        print(\"Breaking\")\n",
    "                        break\n",
    "            #print(\"%s step\" % t)            \n",
    "            agent.step(states, actions, rewards, next_states, dones) # Teach the agent \n",
    "            scores += rewards                                  # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "\n",
    "        #print(\"Steps in rollout: {}\".format(t+1))        \n",
    "        return self.mean_last_100    \n",
    "\n",
    "    def end_episode(self, agent_index, score):\n",
    "        self.episode_scores[agent_index].append(score)   # Save the reward they accumulated in the episode\n",
    "        self.episodes_finished +=1\n",
    "        if (self.episodes_finished % num_agents) == 0:\n",
    "            self.num_episodes += 1\n",
    "            total_over_agents = 0\n",
    "            for i in range(num_agents):\n",
    "                total_over_agents += self.episode_scores[i][-1]\n",
    "            mean_score_over_agents = total_over_agents / num_agents    \n",
    "            self.last_100_scores.append(mean_score_over_agents)\n",
    "            self.mean_scores.append(mean_score_over_agents)\n",
    "            print(\"Finished %s episodes (%s cycles). mean_score_over_agents %s trailing %s\" % (self.num_episodes, (self.episodes_finished/num_agents), mean_score_over_agents, np.mean(self.last_100_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Policy\n",
    "\n",
    "Running the code in the cell below trains the policy, attempting to reach the target treailing avewrage value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reach 100 episode trailing average of 2.00 in under 300 episodes.\n",
      "Rollout length: 250\n",
      "GRADIENT_CLIP 0.75\n",
      "PPO_RATIO_CLIP 0.1\n",
      "GAE_LAMBDA 0.95\n",
      "Finished 1 episodes (1.0 cycles). mean_score_over_agents 0.07199999839067459 trailing 0.07199999839067459\n",
      "Finished 2 episodes (2.0 cycles). mean_score_over_agents 0.11049999753013254 trailing 0.09124999796040356\n",
      "Finished 3 episodes (3.0 cycles). mean_score_over_agents 0.23999999463558197 trailing 0.14083333018546304\n",
      "Finished 4 episodes (4.0 cycles). mean_score_over_agents 0.33699999246746304 trailing 0.18987499575596303\n",
      "Finished 5 episodes (5.0 cycles). mean_score_over_agents 0.5894999868236482 trailing 0.26979999396950005\n",
      "Finished 6 episodes (6.0 cycles). mean_score_over_agents 0.7804999825544655 trailing 0.35491665873366096\n",
      "Finished 7 episodes (7.0 cycles). mean_score_over_agents 0.9419999789446593 trailing 0.4387857044780893\n",
      "Finished 8 episodes (8.0 cycles). mean_score_over_agents 0.6924999845214188 trailing 0.47049998948350547\n",
      "Finished 9 episodes (9.0 cycles). mean_score_over_agents 1.0974999754689634 trailing 0.5401666545930008\n",
      "Finished 10 episodes (10.0 cycles). mean_score_over_agents 1.2014999731443823 trailing 0.6062999864481389\n",
      "Finished 11 episodes (11.0 cycles). mean_score_over_agents 1.2189999727532268 trailing 0.6619999852031468\n",
      "Finished 12 episodes (12.0 cycles). mean_score_over_agents 1.126499974820763 trailing 0.7007083176712815\n",
      "Finished 13 episodes (13.0 cycles). mean_score_over_agents 1.118999974988401 trailing 0.7328845990033677\n",
      "Finished 14 episodes (14.0 cycles). mean_score_over_agents 1.3724999693222344 trailing 0.7785714111690011\n",
      "Finished 15 episodes (15.0 cycles). mean_score_over_agents 1.5004999664612115 trailing 0.826699981521815\n",
      "Finished 16 episodes (16.0 cycles). mean_score_over_agents 1.7054999618791045 trailing 0.8816249802941456\n",
      "Finished 17 episodes (17.0 cycles). mean_score_over_agents 1.3264999703504146 trailing 0.9077940973562791\n",
      "Finished 18 episodes (18.0 cycles). mean_score_over_agents 1.4944999665953218 trailing 0.9403888678695592\n",
      "Finished 19 episodes (19.0 cycles). mean_score_over_agents 1.6544999630190431 trailing 0.9779736623511109\n",
      "Finished 20 episodes (20.0 cycles). mean_score_over_agents 1.9799999557435513 trailing 1.028074977020733\n",
      "Finished 21 episodes (21.0 cycles). mean_score_over_agents 1.92049995707348 trailing 1.0705714046422923\n",
      "Finished 22 episodes (22.0 cycles). mean_score_over_agents 1.8029999596998096 trailing 1.1038636116903613\n",
      "Finished 23 episodes (23.0 cycles). mean_score_over_agents 2.6244999413378536 trailing 1.1699782347185133\n",
      "Finished 24 episodes (24.0 cycles). mean_score_over_agents 1.889999957755208 trailing 1.1999791398450421\n",
      "Finished 25 episodes (25.0 cycles). mean_score_over_agents 2.318499948177487 trailing 1.24471997217834\n",
      "Finished 26 episodes (26.0 cycles). mean_score_over_agents 1.8519999586045741 trailing 1.268076894733195\n",
      "Finished 27 episodes (27.0 cycles). mean_score_over_agents 2.2329999500885607 trailing 1.3038147856722828\n",
      "Finished 28 episodes (28.0 cycles). mean_score_over_agents 2.4484999452717604 trailing 1.3446963985151215\n",
      "Finished 29 episodes (29.0 cycles). mean_score_over_agents 2.4419999454170465 trailing 1.382534451856567\n",
      "Finished 30 episodes (30.0 cycles). mean_score_over_agents 2.6609999405220153 trailing 1.4251499681454154\n",
      "Finished 31 episodes (31.0 cycles). mean_score_over_agents 2.4039999462664126 trailing 1.456725773891254\n",
      "Finished 32 episodes (32.0 cycles). mean_score_over_agents 2.7049999395385385 trailing 1.4957343415677316\n",
      "Finished 33 episodes (33.0 cycles). mean_score_over_agents 2.67849994013086 trailing 1.531575723342372\n",
      "Finished 34 episodes (34.0 cycles). mean_score_over_agents 2.8724999357946217 trailing 1.571014670767438\n",
      "Finished 35 episodes (35.0 cycles). mean_score_over_agents 3.167999929189682 trailing 1.6166428210080737\n",
      "Finished 36 episodes (36.0 cycles). mean_score_over_agents 2.663499940466136 trailing 1.6457221854374644\n",
      "Finished 37 episodes (37.0 cycles). mean_score_over_agents 2.7859999377280475 trailing 1.6765405030669396\n",
      "Finished 38 episodes (38.0 cycles). mean_score_over_agents 2.78149993782863 trailing 1.7056183829290894\n",
      "Finished 39 episodes (39.0 cycles). mean_score_over_agents 2.641999940946698 trailing 1.7296281664680027\n",
      "Finished 40 episodes (40.0 cycles). mean_score_over_agents 2.8234999368898572 trailing 1.7569749607285483\n",
      "Finished 41 episodes (41.0 cycles). mean_score_over_agents 3.1999999284744263 trailing 1.7921706916491795\n",
      "Finished 42 episodes (42.0 cycles). mean_score_over_agents 3.3509999250993134 trailing 1.8292856733979923\n",
      "Finished 43 episodes (43.0 cycles). mean_score_over_agents 3.3564999249763785 trailing 1.8648022838998153\n",
      "Finished 44 episodes (44.0 cycles). mean_score_over_agents 3.075499931257218 trailing 1.8923181395215742\n",
      "Finished 45 episodes (45.0 cycles). mean_score_over_agents 3.380999924428761 trailing 1.9253999569639562\n",
      "Finished 46 episodes (46.0 cycles). mean_score_over_agents 2.80449993731454 trailing 1.9445108261020125\n",
      "Finished 47 episodes (47.0 cycles). mean_score_over_agents 3.0274999323301017 trailing 1.9675531475111208\n",
      "Finished 48 episodes (48.0 cycles). mean_score_over_agents 3.604999919421971 trailing 2.00166662192593\n",
      "Breaking\n",
      "Reached target! mean_last_100 2.00166662192593\n",
      "======== Saving weights ==========\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run time：989.952100s\n"
     ]
    }
   ],
   "source": [
    "agent = MasterAgent(num_agents, state_size=state_size, action_size=action_size, seed=0)\n",
    "import time\n",
    "session = TrainingSession(num_agents)\n",
    "start_time = time.time()\n",
    "scores = session.train_ppo(agent, 2)   # Do the training\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training run time：{:.6f}s\".format(run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Policy\n",
    "The code below runs the policy that has previously been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run_ppo(agent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
