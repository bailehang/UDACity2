# Project Report: Continuous-control

![image-20230620225457569](png\image-20230620225457569.png)

## Project details

This project is part of the Udacity Deep Reinforcement Learning nanodegree.
The goal of this project is to solve the Reacher environment. In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.

The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.

The environment is considered solved if a reward of +100 is obtain for 30 consecutive episodes.

Two methods are used and compared:

- DDPG: an actor-critic algorithm, the Deep Deterministic Policy Gradients algorithm

- PPO: an actor-critic algorithm, the Proximal Policy Optimization algorithm

  

## Getting started

1. `conda env create -n drlnd --file drlnd.yml`,  Install the necessary environment.

2. `conda activate drlnd`

3. Then,  find directory and unzip the `Reacher_Windows_x86_64.zip` to the current directory

4. `jupyter notebook Continuous_Control.ipynb`

5. The training can be run directly in the notebook

   

## Algorithm

PPO (Proximal Policy Optimization) and DDPG (Deep Deterministic Policy Gradient) are both commonly used algorithms in deep reinforcement learning, but they differ in their design and application scenarios. Here are some of the main differences between them:

1. Algorithm type: PPO is a policy-based reinforcement learning algorithm, while DDPG is a value-based reinforcement learning algorithm.
2. Network architecture: PPO typically uses a single neural network to learn the policy, while DDPG typically uses two neural networks to learn the action-value function and the policy.
3. Policy update method: PPO uses a method called "proximal policy optimization" to update the policy, which ensures that each update does not deviate too far from the original policy, thereby improving stability. In contrast, DDPG uses gradient-based updates to directly update the policy network.
4. Reward function: PPO is suitable for both continuous and discrete action spaces, and the reward function is usually specified, while DDPG is more suitable for continuous action spaces, and the reward function is usually generated by the environment.
5. Algorithm advantages: PPO has good convergence and robustness when dealing with highly unstable environments, and is suitable for high-dimensional state spaces and discrete or continuous action spaces; DDPG performs well in handling continuous action spaces and can handle tasks with continuous temporal and spatial properties.

In general, PPO is more suitable for handling highly unstable environments with discrete or continuous action spaces, while DDPG is more suitable for handling continuous action spaces. However, in practical applications, the specific choice of algorithm should be determined based on the characteristics and requirements of the specific problem.



## Plot of Rewards

![20230620231554](png\20230620231554.png)

## Ideas for Future Work

- It can be seen there seam to be some cases where the DDPG agent algorithms fail to learn. Maybe we can control the learning effect better ?
- Using gpu to accelerate algorithms has no obvious results, even with larger batches. Maybe some code control performed in parallel with the gpu can be accelerated?
